{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Mk0AWYHC8Q9"
   },
   "outputs": [],
   "source": [
    "#get data from Phillip's google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0E-qLXCiDmP"
   },
   "outputs": [],
   "source": [
    "#path of data\n",
    "gdrive_path = '/content/gdrive/My Drive/Sentiment Analysis Data/'\n",
    "#following line opens amazon data\n",
    "#data is split into sentence \\t score \\n\n",
    "#with open(f'{gdrive_path}amazon_cells_labelled.txt', 'r') as file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLOhN-eYmJ6P",
    "outputId": "21ceb31a-44a3-4040-e61a-1eb1d38c6d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence  score\n",
      "0    So there is no way for me to plug it in here i...      0\n",
      "1                          Good case, Excellent value.      1\n",
      "2                               Great for the jawbone.      1\n",
      "3    Tied to charger for conversations lasting more...      0\n",
      "4                                    The mic is great.      1\n",
      "..                                                 ...    ...\n",
      "743  I just got bored watching Jessice Lange take h...      0\n",
      "744  Unfortunately, any virtue in this film's produ...      0\n",
      "745                   In a word, it is embarrassing.        0\n",
      "746                               Exceptionally bad!        0\n",
      "747  All in all its an insult to one's intelligence...      0\n",
      "\n",
      "[2748 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS \n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#take list of filepaths to get data\n",
    "filepaths = {'amazon': 'amazon_cells_labelled.txt',\n",
    "               'yelp' : 'yelp_labelled.txt',\n",
    "               'imdb': 'imdb_labelled.txt'}\n",
    "\n",
    "#populate dataframes with data, separating sentences from the scores\n",
    "dfs = []\n",
    "for source, path in filepaths.items():\n",
    "  df = pd.read_csv(path, names = ['sentence', 'score'], sep = '\\t')\n",
    "  #df['source'] = source\n",
    "  dfs.append(df)\n",
    "df = pd.concat(dfs)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y7XWAOPDvLOp",
    "outputId": "a837a609-628b-44c9-9345-1615696a2b3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence  score\n",
      "0     [So, way, plug, US, unless, I, go, converter, .]      0\n",
      "1                 [Good, case, ,, Excellent, value, .]      1\n",
      "2                                  [Great, jawbone, .]      1\n",
      "3    [Tied, charger, conversations, lasting, 45, mi...      0\n",
      "4                                 [The, mic, great, .]      1\n",
      "..                                                 ...    ...\n",
      "743  [I, got, bored, watching, Jessice, Lange, take...      0\n",
      "744  [Unfortunately, ,, virtue, film, 's, productio...      0\n",
      "745                     [In, word, ,, embarrassing, .]      0\n",
      "746                            [Exceptionally, bad, !]      0\n",
      "747  [All, insult, one, 's, intelligence, huge, was...      0\n",
      "\n",
      "[2748 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords and lemmatize\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#sentences = df['sentence'].values\n",
    "modified_sentences = []\n",
    "\n",
    "\n",
    "for i in range(df['sentence'].size):\n",
    "  modified_sentence = []\n",
    "  words = word_tokenize(df.iloc[i,0])\n",
    "  for word in words:\n",
    "    if word not in stops:\n",
    "      lemmatizer.lemmatize(word)\n",
    "      modified_sentence.append(word)\n",
    "      #sentence = ' '.join(modified_sentence)\n",
    "      df.iat[i,0] = modified_sentence\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lzbSjVXZMR72",
    "outputId": "65c42a93-05f9-45c3-aa68-b2b8cbf00a63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#vectorize text\n",
    "vectorizer = text.TfidfVectorizer(min_df = 1, sublinear_tf = True, use_idf = True, ngram_range = (1, 2))\n",
    "for i in range(df['sentence'].size):\n",
    "  X = vectorizer.fit_transform(df.iloc[i,0])\n",
    "  df.iat[i,0] = X.toarray()\n",
    "print(df.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PWycGQ4eT3L0",
    "outputId": "4b99b216-309a-4d9c-f247-c528e3844ccc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311    [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0....\n",
      "26     [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0....\n",
      "675    [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0....\n",
      "23     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "474    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "                             ...                        \n",
      "627    [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0....\n",
      "650    [[0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0....\n",
      "860    [[0.0, 1.0, 0.0], [0.0, 0.0, 1.0], [1.0, 0.0, ...\n",
      "610    [[0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0], [...\n",
      "281     [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 0.0]]\n",
      "Name: sentence, Length: 1841, dtype: object\n",
      "311    0\n",
      "26     0\n",
      "675    0\n",
      "23     1\n",
      "474    0\n",
      "      ..\n",
      "627    0\n",
      "650    0\n",
      "860    0\n",
      "610    0\n",
      "281    1\n",
      "Name: score, Length: 1841, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['sentence'], df['score'], test_size = 0.33, shuffle = True)\n",
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "48Pv26npT4gt",
    "outputId": "b81e7dcc-d756-4488-c7d1-9ae84d6f35a7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-f1c6b5c59cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f1_macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LR' is not defined"
     ]
    }
   ],
   "source": [
    "log_reg = LR()\n",
    "labels = cross_val_score(log_reg, df['sentence'], df['score'], cv= 10, scoring = 'f1_macro')\n",
    "print(labels.mean())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Sentiment Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
